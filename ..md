# Chapter 2

### 2.1 understanding word embeddings
embeddings are just convertin non numerical data (such as text) into numeric data - this is called word embedding. map each word to a numeric data.
Also there's sentence or paragraph embeddings - mapping sentences/paras to numeric data (used in RAG btw)

word2vec is an embedding model that would figure out the word embeddings by predicting the context of that word given the target or by predicting the target given the context (exact details idk man)  
words that appear in similar contexts tend to have similar meaning  
Pretrained models are used to generate embeddings for ML models, llms however have their own embeddings in the input layer and are updated during training. Well why not just use word2vec?? here's the thing if you train your own embedding model then you have your data fitting into your context - it will match more with the context of you training data instead of something generic that word2vec will likely produce

Even the smallest gpt2 models with 117M and 125M parameters use an embedding size of 768 dimensions. GPT3 with 175b parameters uses 12,288 dimensions (sheesh)

Summary:
- Embeddings is the process of converting words/sentences(depends) into vectors
- word2vec is a pretrained neural network normally used to generate embeddings for ml models
- llms usually do not use word2vec. They have their own embeddings in the input layer. Think of llms as one neural network - everything is happening within

### 2.2 tokenizing text
now forget embeddings for a while, you need to tokenize text first.
- each word is a token, each special character is a token
- each whitespace is skipped, all others are split into tokens\

so you:
- downloaded the verdict, read it
- to understand tokenization, you write a regex that will split words on each whitespace (to get singular words)
- some words are connected with a punctuation so you're now splitting each punctuation mark too. 
- now loop through each and remove whitespace tokens for the simplicity 

### 2.3 converting tokens into token ids
In this step you will convert all tokens into token ids. This step comes before converting the token ids into embedding vectors. 
So far;
- split text into tokens
- convert into token ids 
- then comes 'embeddings'  
To convert into token ids; 
- just grab all words, create a set to remove duplicates
- use enumerate to assign a number to each and create a dict object - call it `vocab`
- now we'll use this `vocab` to convert all the tokens into token ids. You have a map, now you transform everything according to that map. 
  
![Figure 2.7](md-assets/image.png)  
  

At this point, you have a list of token_ids adjacent to each token so you can convert your original list of tokens into token_ids. 
Now well you need the opposite as well so you create a tokenizer class with an `encode` method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary and a `decode` method that carries out the reverse integer-to-string mapping to convert the token ids back into text. 
you create the `SimpleTokenizerV1` class for this:

```Python
class SimpleTokenizerV1:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = { i:s for s,i in vocab.items() }

    def encode(self,text):
        preprocessed = re.split(r'([,.?_!"()\']|--|\s)', text)
        preprocessed = [item.strip() for item in preprocessed if item.strip()]
        ids = [self.str_to_int[s] for s in preprocessed]
        return ids

    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])
        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)
        return text
    
```

Now try using a word which is not available in the vocab - (book used 'hello') and you'll get an error because the vocab does not know what you're saying - as that word wasn't in `the-verdict.txt`



### 2.4 Special context tokens
In 2.3, you encountered an error because the token which was to be encoded wasn't available in `vocab` so now enters `SimpleTokenizerV2` which will encode the words that aren't in `vocab` with a special token `<| unk |>`.

Also whenever concatenating text from different sources, use the `<| end of text |>` special token so that the model knows that these texts are separate - this will help in better 'next-word' prediction thingy as you will be separating the context.   
Now, you take your existing vocab and add two more tokens in it, `<| unk |>` and `<| end of text |>`. then create tokenizerv2 as following:  

```Python
class SimpleTokenizerV2:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = {i:s for s,i in vocab.items()}
    
    def encode(self, text):
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)
        preprocessed = [item.strip() for item in preprocessed if item.strip()]

        preprocessed = [item if item in self.str_to_int else "<|unk|>" for item in preprocessed]
        ids = [self.str_to_int[s] for s in preprocessed]
        return ids
    
    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])
        text = re.sub(r'\s+([,.:;?!"()\'])', r'\1', text)
        return text
```
tldr;  
- For words out of vocab, use the <| unk |> token, and <| EOS |> or <| end of text|> for end of a sentence (helps model understand context)
- gpt tokenizer only uses <| end of text |> token for simplicity
- gpt tokenizer also doesn't use <| unk |> token for out of vocab words instead they use byte pair encoding which breaks down words into subwords

### 2.5 Byte Pair Encoding
(book doesn't go deep  in this but i will, lol)  

BPE looks at data then decides how to tokenize them, went ahead and found out the whole algorithm, it's relatively easy

```
for k times:
    - choose the most frequentyly adjacent symbols (suppose t and h are appearing together a lot)
    - replace every adjacent t and h with th - like link them together
```
note that bpe does not need the `<| unk |>` token as it is iteratively figuring out words and assigning tokens unlike the tokenizing approaches of `SimpleTokenizerV2`. If it encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters. 
The ability to break down unknown words into individual characters ensures that the tokenizer and, consequently, the LLM that is trained with it can process any text, even if it contains words that were not present in its training data  
(Normally used with `tiktoken` - library for tokenizing, duh?)


### 2.6 Data Sampling wih a sliding window
Next step is to create input-target pairs.  
Input is the group of words you feed into an llm  
target would be the word that it should predict  

##### Sliding window Approach: 
take a chunk of tokens, create x from 0:lastidx and then a y from 1:lastidx+1

- the first approach is to create two lists with indices as described above
- You then create a GPTDatasetv1, which is in short the same list of token ids except this time it's a tensor (multidimensional array - see fig2.13 again? )

the code here was kinda complicating so just to recap:  
- created a gptdatasetv1, all its doing is creating the input-target pair splits except it is tensors instead of lists. One more extra thing is that it's splitting these into chunks. say you have a text of 1000 tokens. GPTDatasetV1 will convert that into 1000 token ids, split by idk say 10 (which is the attribute `max_length`) so there's 100 splits of 10 token ids each and there's an attribute `stride` which means um say you created input/target pairs for 10 tokens (coz 10 is `max_length` suppose) - after creating a batch of 0-9 tokens, we will move forward by `stride` (suppose `stride` is 5) so you move to token 5 (think 0,1,2,3,4 is 5 tokens so the next is token number 5, i hope im making sense??) then repeat. Now for each of these splits (each split is `max_length` long), you have two tensors : an input tensor and a target tensor. 
Each target tensor is the same as it's input sensors just shifted by 1
- `createdataloaderv1` is just initializing a tiktoken tokenizer and sending it to to the `GPTDatasetV1` class  
  
  
Smaller batch sizes = Noisy model updates, which is why its normal to have batch sizes over atleast 256.  
However,  
Smaller batch sizes = less memory required during training, so batch size is a tradeoff to experiment wd

Also note, batch_size is the length of a tensor and max_length is the number of items in each inner list (coz multidimensioanl array, remember) 



### 2.7 Create token embeddings
- embeddings are vector representation of tokens. 
- In training, you have tokens, you assign a random embedding to all of them then you try predicting the next word in a sentence - this would mostly be wrong so you go back (back propagation) to figure where the error came from then you calculate loss and all to adjust embedding vector values plus some weights now your model is slightly better. repeat this a 1000 times until your model gets better. 
- `vocab_size` and `output_dim` is the dimension of the resulting tensor embedding thingy

page 42 onwards , i dont get it :/


### 2.8 Encoding word positions
The token id remains the same regardless of it's position and regardless of how far apart it is. Now you can think of a problem here, 
imagine two sentences "I dont think this is working" and "I like this", the position of "this" and "i" in both of the sentences has variable lengths but the words in both of these sentences will have the same token id. "this" in sentence 1 and sentence 2 will have the same token id even though they are used in different contexts and you can see how the model will have probs figuring out what words to pay attention to if they just have the same token ids.  

To solve this, there's two broad categories of position-aware embeddings (embeddings unlike the ones described above, they are aware of the "context" so to speak)  

- **Absolute Positional Embeddings**:  
. Absolute Positional Embeddings are directly associated with specific positions in a sequence. 


- **Relative Positional Embeddings**:  
Relative positional embeddings will tell you relationships based on how far apart instead of "at wihch exact position". this will help generalizing better to variable sequence length

(GPT models use ansolute positional embeddings, instead of fixed positional encodings in the original transformer model)