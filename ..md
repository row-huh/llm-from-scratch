### 2.1 understanding word embeddings
embeddings are just convertin non numerical data (such as text) into numeric data - this is called word embedding. map each word to a numeric data.
Also there's sentence or paragraph embeddings - mapping sentences/paras to numeric data (used in RAG btw)

word2vec is an embedding model that would figure out the word embeddings by predicting the context of that word given the target or by predicting the target given the context (exact details idk man)  
words that appear in similar contexts tend to have similar meaning  
Pretrained models are used to generate embeddings for ML models, llms however have their own embeddings in the input layer and are updated during training. Well why not just use word2vec?? here's the thing if you train your own embedding model then you have your data fitting into your context - it will match more with the context of you training data instead of something generic that word2vec will likely produce

Even the smallest gpt2 models with 117M and 125M parameters use an embedding size of 768 dimensions. GPT3 with 175b parameters uses 12,288 dimensions (sheesh)

summary:
- Embeddings is the process of converting words/sentences(depends) into vectors
- word2vec is a pretrained neural network normally used to generate embeddings for ml models
- llms usually do not use 

### 2.2 tokenizing text
- each word is a token, each special character is a token, each whitespace is a token
- exccept each whitespace is skipped, all others are split into tokens

### 2.3 converting tokens into token ids
- just grab all words, create a set to remove duplicates, assign a number to each
- then you can mathematically represent the same words by using their token ids 

### 2.4 Special context tokens
- for words out of vocab, use the <| unk |> token, and <| EOS |> or <| end of text|> for end of a sentence (helps model understand context)
- gpt tokenizer only uses <| end of text |> token for simplicity
- gpt tokenizer also doesn't use <| unk |> token for out of vocab words instead they use byte pair encoding which breaks down words into subwords

### 2.5 Byte Pair Encoding
(book doesn't go deep  in this but i will, lol)  

BPE looks at data then decides how to tokenize them, went ahead and found out the whole algorithm, it's relatively easy

```
for k times:
    - choose the most frequentyly adjacent symbols (suppose t and h are appearing together a lot)
    - replace every adjacent t and h with th - like link them together
```

normally used with `tiktoken` - library for tokenizing, duh?

BPE is better than regex-based simple tokenizer's because if it encounters an unfamiliar word, it cann represent it as a sequence of subword tokens or chars.


### 2.6 Data Sampling wih a sliding window
Next step is to create input-target pairs.  
Input is the group of words you feed into an llm  
target would be the word that it should predict  

##### Sliding window Approach: 
take a chunk of tokens, create x from 0:lastidx and then a y from 1:lastidx+1

- the first approach is to create two lists with indices as described above
- You then create a GPTDatasetv1, which is in short the same list of token ids except this time it's a tensor (multidimensional array - see fig2.13 again? )

the code here was kinda complicating so just to recap:  
- created a gptdatasetv1, all its doing is creating the input-target pair splits except it is tensors instead of lists. One more extra thing is that it's splitting these into chunks. say you have a text of 1000 tokens. GPTDatasetV1 will convert that into 1000 token ids, split by idk say 10 (which is the attribute `max_length`) so there's 100 splits of 10 token ids each and there's an attribute `stride` which means um say you created input/target pairs for 10 tokens (coz 10 is `max_length` suppose) - after creating a batch of 0-9 tokens, we will move forward by `stride` (suppose `stride` is 5) so you move to token 5 (think 0,1,2,3,4 is 5 tokens so the next is token number 5, i hope im making sense??) then repeat. Now for each of these splits (each split is `max_length` long), you have two tensors : an input tensor and a target tensor. 
Each target tensor is the same as it's input sensors just shifted by 1
- `createdataloaderv1` is just initializing a tiktoken tokenizer and sending it to to the `GPTDatasetV1` class  
  
  
Smaller batch sizes = Noisy model updates, which is why its normal to have batch sizes over atleast 256.  
However,  
Smaller batch sizes = less memory required during training, so batch size is a tradeoff to experiment wd

Also note, batch_size is the length of a tensor and max_length is the number of items in each inner list (coz multidimensioanl array, remember) 



### 2.7 Create token embeddings
- embeddings are vector representation of tokens. 
- In training, you have tokens, you assign a random embedding to all of them then you try predicting the next word in a sentence - this would mostly be wrong so you go back (back propagation) to figure where the error came from then you calculate loss and all to adjust embedding vector values plus some weights now your model is slightly better. repeat this a 1000 times until your model gets better. 
- `vocab_size` and `output_dim` is the dimension of the resulting tensor embedding thingy

page 42 onwards , i dont get it :/


### 2.8 Encoding word positions
The token id remains the same regardless of it's position and regardless of how far apart it is. Now you can think of a problem here, 
imagine two sentences "I dont think this is working" and "I like this", the position of "this" and "i" in both of the sentences has variable lengths but the words in both of these sentences will have the same token id. "this" in sentence 1 and sentence 2 will have the same token id even though they are used in different contexts and you can see how the model will have probs figuring out what words to pay attention to if they just have the same token ids.  

To solve this, there's two broad categories of position-aware embeddings (embeddings unlike the ones described above, they are aware of the "context" so to speak)  

- **Absolute Positional Embeddings**:  
. Absolute Positional Embeddings are directly associated with specific positions in a sequence. 


- **Relative Positional Embeddings**:  
Relative positional embeddings will tell you relationships based on how far apart instead of "at wihch exact position". this will help generalizing better to variable sequence length

(GPT models use ansolute positional embeddings, instead of fixed positional encodings in the original transformer model)