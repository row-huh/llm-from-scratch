### 2.1 understanding word embeddings
embeddings are just convertin non numerica data (such as text) into numeric data - this is called word embedding. map each word to a numeric data.
Also there's sentence or paragraph embeddings - mapping sentences/paras to numeric data (used in RAG btw)

### 2.2 tokenizing text
- each word is a token, each special character is a token, each whitespace is a token
- exccept each whitespace is skipped, all others are split into tokens

### 2.3 converting tokens into token ids
- just grab all words, create a set to remove duplicates, assign a number to each
- then you can mathematically represent the same words by using their token ids

### 2.4 Special context tokens
- for words out of vocab, use the <| unk |> token, and <| EOS |> or <| end of text|> for end of a sentence (helps model understand context)
- gpt tokenizer only uses <| end of text |> token for simplicity
- gpt tokenizer also doesn't use <| unk |> token for out of vocab words instead they use byte pair encoding which breaks down words into subwords

### 2.5 Byte Pair Encoding
(book doesn't go deep  in this but i will, lol)  

BPE ooks at data then decides how to tokenize them, went ahead and found out the whole algorithm, it's relatively easy

```
for k times:
    - choose the most frequentyly adjacent symbols (suppose t and h are appearing together a lot)
    - replace every adjacent t and h with th - like link them together
```

normally used with `tiktoken` - library for tokenizing, duh?