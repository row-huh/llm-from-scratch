### 2.1 understanding word embeddings
embeddings are just convertin non numerica data (such as text) into numeric data - this is called word embedding. map each word to a numeric data.
Also there's sentence or paragraph embeddings - mapping sentences/paras to numeric data (used in RAG btw)

### 2.2 tokenizing text
- each word is a token, each special character is a token, each whitespace is a token
- exccept each whitespace is skipped, all others are split into tokens

### 2.3 converting tokens into token ids
- just grab all words, create a set to remove duplicates, assign a number to each
- then you can mathematically represent the same words by using their token ids

### 2.4 Special context tokens
- for words out of vocab, use the <| unk |> token, and <| EOS |> or <| end of text|> for end of a sentence (helps model understand context)
- gpt tokenizer only uses <| end of text |> token for simplicity
- gpt tokenizer also doesn't use <| unk |> token for out of vocab words instead they use byte pair encoding which breaks down words into subwords

### 2.5 Byte Pair Encoding
(book doesn't go deep  in this but i will, lol)  

BPE looks at data then decides how to tokenize them, went ahead and found out the whole algorithm, it's relatively easy

```
for k times:
    - choose the most frequentyly adjacent symbols (suppose t and h are appearing together a lot)
    - replace every adjacent t and h with th - like link them together
```

normally used with `tiktoken` - library for tokenizing, duh?

BPE is better than regex-based simple tokenizer's because if it encounters an unfamiliar word, it cann represent it as a sequence of subword tokens or chars.


### 2.6 Data Sampling wih a sliding window
Next step is to create input-target pairs.  
Input is the group of words you feed into an llm  
target would be the word that it should predict  

##### Sliding window Approach: 
take a chunk of tokens, create x from 0:lastidx and then a y from 1:lastidx+1

- the first approach is to create two lists with indices as described above
- You then create a GPTDatasetv1, which is in short the same list of token ids except this time it's a tensor (multidimensional array - see fig2.13 again? )

the code here was kinda complicating so just to recap:  
- created a gptdatasetv1, all its doing is creating the input-target pair splits except it is tensors instead of lists. One more extra thing is that it's splitting these into chunks. say you have a text of 1000 tokens. GPTDatasetV1 will convert that into 1000 token ids, split by idk say 10 (which is the attribute `max_length`) so there's 100 splits of 10 token ids each and there's an attribute `stride` which means um say you created input/target pairs for 10 tokens (coz 10 is `max_length` suppose) - after creating a batch of 0-9 tokens, we will move forward by `stride` (suppose `stride` is 5) so you move to token 5 (think 0,1,2,3,4 is 5 tokens so the next is token number 5, i hope im making sense??) then repeat. Now for each of these splits (each split is `max_length` long), you have two tensors : an input tensor and a target tensor. 
Each target tensor is the same as it's input sensors just shifted by 1
- `createdataloaderv1` is just initializing a tiktoken tokenizer and sending it to to the `GPTDatasetV1` class  
  
  
Smaller batch sizes = Noisy model updates, which is why its normal to have batch sizes over atleast 256.  
However,  
Smaller batch sizes = less memory required during training, so batch size is a tradeoff to experiment wd

Also note, batch_size is the length of a tensor and max_length is the number of items in each inner list (coz multidimensioanl array, remember) 



### 2.7 Create token embeddings
(gotta read more into neural networks wd back propagation brb)