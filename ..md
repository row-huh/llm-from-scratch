### 2.1 understanding word embeddings
embeddings are just convertin non numerica data (such as text) into numeric data - this is called word embedding. map each word to a numeric data.
Also there's sentence or paragraph embeddings - mapping sentences/paras to numeric data (used in RAG btw)

### 2.2 tokenizing text
- each word is a token, each special character is a token, each whitespace is a token
- exccept each whitespace is skipped, all others are split into tokens

### 2.3 converting tokens into token ids
- just grab all words, create a set to remove duplicates, assign a number to each
- then you can mathematically represent the same words by using their token ids

### 2.4 Special context tokens
- for words out of vocab, use the <| unk |> token, and <| EOS |> or <| end of text|> for end of a sentence (helps model understand context)
- gpt tokenizer only uses <| end of text |> token for simplicity
- gpt tokenizer also doesn't use <| unk |> token for out of vocab words instead they use byte pair encoding which breaks down words into subwords

### 2.5 Byte Pair Encoding
(book doesn't go deep  in this but i will, lol)  

BPE looks at data then decides how to tokenize them, went ahead and found out the whole algorithm, it's relatively easy

```
for k times:
    - choose the most frequentyly adjacent symbols (suppose t and h are appearing together a lot)
    - replace every adjacent t and h with th - like link them together
```

normally used with `tiktoken` - library for tokenizing, duh?

BPE is better than regex-based simple tokenizer's because if it encounters an unfamiliar word, it cann represent it as a sequence of subword tokens or chars.


### 2.6 Data Sampling wih a sliding window
Next step is to create input-target pairs.  
Input is the group of words you feed into an llm  
target would be the word that it should predict  

##### Sliding window Approach: 
take a chunk of tokens, create x from 0:lastidx and then a y from 1:lastidx+1

- the first approach is to create two lists with indices as described above
- You then create a GPTDatasetv1, which is in short the same list of token ids except this time it's a tensor (multidimensional array - see fig2.13 again? )
